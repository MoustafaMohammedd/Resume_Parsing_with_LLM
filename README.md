# ğŸ“„ Resume Parsing with LLM

A **resume parsing system powered by Large Language Models (LLMs)**.
This project enables users to upload resumes in **PDF or DOCX format**, automatically extracts key details such as **contact information, skills, education, work experience, and projects**, and returns them in a **well-structured JSON format**.

## ğŸ”¹ Why This Project?

Traditional resume parsing solutions rely on **regex rules** or **basic NLP techniques**, which often fail due to the wide variety of resume formats and layouts. By leveraging **LLMs (Large Language Models)**, this system provides:

* âœ… **Higher Accuracy** â†’ Handles diverse resume formats without breaking on layout differences.
* âœ… **Structured Output** â†’ Returns information in a **consistent JSON schema**, making it easy to store, analyze, or integrate with other systems.
* âœ… **Multi-format Support** â†’ Works with `.pdf` and `.docx` files, with potential to extend to images (OCR).
* âœ… **ATS-Friendly Data** â†’ Prepares resumes for **Applicant Tracking Systems (ATS)** by organizing key details cleanly.
* âœ… **Scalability** â†’ Can be used for **bulk resume processing** in recruitment pipelines.
* âœ… **Extensibility** â†’ New fields (e.g., certifications, languages, awards) can be added easily in the JSON schema.

## ğŸ”¹ Benefits

* ğŸ“‚ **Recruiters & HR** â†’ Save time by instantly extracting and structuring candidate details for faster filtering and matching.
* ğŸ§‘â€ğŸ’» **Developers & Researchers** â†’ Get a ready-to-use parsing pipeline that can be integrated into **job boards, HR systems, or analytics dashboards**.
* ğŸ¢ **Organizations** â†’ Improve efficiency in **talent acquisition** by automating manual resume screening.
* ğŸ” **Data Analysis** â†’ Structured resume data can be used for **skills analytics, hiring trends, or workforce planning**.

---

## âœ¨ Features

* ğŸ“‚ **Multi-format support** â†’ Upload resumes in `.pdf` or `.docx`
* ğŸ§  **LLM-powered extraction** â†’ Uses LangChain + OpenAI models
* âœ… **Validated JSON output** â†’ Ensures consistent schema
* âš¡ **FastAPI backend** â†’ REST API endpoints for parsing & retrieval
* ğŸ§ª **Tests included** â†’ Testing LLM handler & parser pipeline
* ğŸ“Š **Notebook experiments** â†’ Test prompts and parsing logic in `notebooks/`

---

## ğŸ“‚ Project Structure

```
Resume_Parsing_with_LLM/
â”‚â”€â”€ data/                          # Sample resumes
â”‚   â””â”€â”€ Mostafa-Mohamed-Mostafa-Resume-3p.pdf
â”‚
â”‚â”€â”€ notebooks/                     # Experimentation
â”‚   â””â”€â”€ testing_llm.ipynb
â”‚
â”‚â”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ config.py                  # File path & configs
â”‚   â”œâ”€â”€ llm_handler.py             # LLM calls + JSON validation
â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry
â”‚   â”œâ”€â”€ parser.py                  # Pipeline: load â†’ ask LLM â†’ validate â†’ save
â”‚   â”œâ”€â”€ schema.py                  # Prompt templates & schema
â”‚   â””â”€â”€ utils.py                   # File loaders, save/load JSON
â”‚
â”‚â”€â”€ tests/                         # Tests
â”‚   â”œâ”€â”€ test_llm_handler.py
â”‚   â””â”€â”€ test_parser.py
â”‚
â”‚â”€â”€ requirements.txt               # Python dependencies
â”‚â”€â”€ .env.example                   # Example environment variables
â””â”€â”€ README.md                      # Documentation
```

---

## âš™ï¸ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/MoustafaMohammedd/Resume_Parsing_with_LLM.git
   cd Resume_Parsing_with_LLM
   ```

2. Create and activate a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the project root based on `.env.example`:

```ini
API_KEY="your_api_key_here"
BASE_URL="https://api.openai.com/v1"
MODEL_NAME="openai/gpt-oss-20b:free"
```

---

## â–¶ï¸ Running the API

Start the FastAPI server with Uvicorn:

```bash
uvicorn src.main:app --reload
```

* API runs at: [http://127.0.0.1:8000](http://127.0.0.1:8000)
* Swagger UI (interactive docs): [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ“Œ API Endpoints

### 1ï¸âƒ£ Root Check

`GET /`
Returns a welcome message.

### 2ï¸âƒ£ Process Resume File

`POST /process_file`

**Request Body:**

```json
{
  "file_path": "data/Mostafa-Mohamed-Mostafa-Resume-3p.pdf"
}
```

**Response:**

```json
{
  "message": "File processed successfully.",
  "output_file": "parsed_resume/data/Mostafa-Mohamed-Mostafa-Resume-3p.json"
}
```

### 3ï¸âƒ£ Get Parsed Data

`POST /get_parsed_data`

**Request Body:**

```json
{
  "message": "File processed successfully.",
  "output_file": "parsed_resume/data/Mostafa-Mohamed-Mostafa-Resume-3p.json"
}
```

**Response:**

```json
{
  "data": {
    "name": "Mostafa Mohamed Mostafa",
    "contact": {
      "email": "mostafaelkazaz149@gmail.com",
      "phone": "+201025823935"
    },
    "skills": ["Python", "Machine Learning", "NLP"],
    "education": [...],
    "experience": [...]
  }
}
```

---

## ğŸš€ Future Enhancements

* ğŸŒ Add **UI** for easy upload and visualization
* ğŸ“ Support for more file formats (`.txt`, `.jpg` scanned resumes)
* ğŸ“Š Integrate with ATS (Applicant Tracking System) for job matching
* ğŸ” Improve entity extraction with fine-tuned LLM

---
